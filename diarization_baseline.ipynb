{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7js94bngg30"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2OENoC1PLWf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://files.deeppavlov.ai/field-matters/releases/demo/sound.zip\n",
    "!wget -c https://files.deeppavlov.ai/field-matters/releases/demo/dia_data.csv\n",
    "!unzip sound.zip #your audios here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b9mIJG6eg2x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dia_data.csv') #your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOEgS1aPifVI"
   },
   "outputs": [],
   "source": [
    "df['fpath'] = './audio_to_release/' + df['lang'] + '/' + df['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0w_T09MjDIs"
   },
   "outputs": [],
   "source": [
    "df['fpath'] = df['fpath'].apply(lambda x: x.replace(' ', '_')) #removing spaces in dataset's paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuEpf8lSOxYY"
   },
   "outputs": [],
   "source": [
    "all_paths = df['fpath']\n",
    "all_paths = list(set(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJK3XutQNurQ"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index() #adding indexes(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cygiPVzEXYP"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df.drop(['speaker', 'source', 'lang', 'index'], axis=1).groupby('fpath').apply(dict)).reset_index()\n",
    "df2['coord_start'] = df2[0].apply(lambda x: list(x['start'])[0])\n",
    "df2['coord_end'] = df2[0].apply(lambda x: list(x['end'])[-1])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxvE1zufM6bv"
   },
   "outputs": [],
   "source": [
    "def cutter(row): #cutting files accroding to timecodes\n",
    "    fpath, start, end, index = row[\"fpath\"], row[\"coord_start\"], row[\"coord_end\"], row[\"fpath\"]\n",
    "    !ffmpeg -n -i {fpath} -ss {str(start)} -to {str(end)} -ar 16000 \\\n",
    "     {str(index)}.wav\n",
    "\n",
    "    \n",
    "df2.progress_apply(cutter, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twI1rrcGS0EQ"
   },
   "outputs": [],
   "source": [
    "dirs = os.listdir(\"./audio_to_release\") \n",
    "for i in range(len(dirs)): #all directories\n",
    "    dirs[i] = \"./audio_to_release/\" + dirs[i] \n",
    "\n",
    "def remover(directory): #getting rid of original files\n",
    "    for item in os.listdir(directory):\n",
    "        if (not '.wav.wav' in item) and (not '.mp4.wav' in item) and (not '.WAV.wav' in item):\n",
    "            os.remove(os.path.join(directory, item))\n",
    "            \n",
    "for item in dirs:\n",
    "    remover(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBkC-vblTNvp"
   },
   "outputs": [],
   "source": [
    "def replacer(fpath): #new paths in dataframe's column 'fpath' according to new file names\n",
    "    fpath = fpath.replace('.wav', '.wav.wav').replace('.mp4', '.mp4.wav').replace('.WAV', '.WAV.wav')\n",
    "    return fpath\n",
    "\n",
    "df['fpath'] = df['fpath'].apply(replacer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWFW0BFhTiIQ"
   },
   "outputs": [],
   "source": [
    "all_paths = set(list(df['fpath']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk3JBaGyTzYU"
   },
   "source": [
    "# New shape for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole section is dedicated to reformating the dataset. After manipulations below, anntoation for speakers can be obtained (for each recording)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "annRzqv8g2cY"
   },
   "outputs": [],
   "source": [
    "path_dict = {}\n",
    "for idx, row in df.iterrows():\n",
    "    if row['fpath'] in path_dict:\n",
    "        if row['speaker'] in path_dict[row['fpath']]:\n",
    "            path_dict[row['fpath']][row['speaker']]['start'].append(row['start'])\n",
    "            path_dict[row['fpath']][row['speaker']]['end'].append(row['end'])\n",
    "        else:\n",
    "            path_dict[row['fpath']][row['speaker']] = {'start': [row['start']], 'end' : [row['end']]}\n",
    "    else:\n",
    "        path_dict[row['fpath']] = {row['speaker']: {'start': [row['start']], 'end' : [row['end']]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqUZRV8OoR0e"
   },
   "outputs": [],
   "source": [
    "path_dict_new = {}\n",
    "for path in path_dict.keys():\n",
    "    path_dict_new[path] = {}\n",
    "    for speaker in path_dict[path].keys():\n",
    "        path_dict_new[path][speaker] = {}\n",
    "        times = path_dict[path][speaker]\n",
    "        for i in range(1, len(times['start'])+1):\n",
    "            path_dict_new[path][speaker][i] = [times['start'][i-1], times['end'][i-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcV9jAcwtWe4"
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(path_dict_new.items(), columns=['file_path', 'source_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['speakers'] = new_df['source_data'].apply(len) #determining the number of speakers for each audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA0QHcUDu6cU"
   },
   "outputs": [],
   "source": [
    "new_df.to_csv('new_shape.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "625yKlHzgrI6"
   },
   "source": [
    "# Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SadPNhU4gtPm"
   },
   "outputs": [],
   "source": [
    "!pip install pyannote.audio pyannote.core\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTn33zqvv673"
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.features import RawAudio\n",
    "from IPython.display import Audio\n",
    "from sklearn.cluster import KMeans\n",
    "from pyannote.core import Segment, notebook\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16dHYShJfpWT"
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.features import RawAudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C43kP6y3wkZe"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipeline = torch.hub.load('pyannote/pyannote-audio', 'dia', device=device) #using viedo card\n",
    "\n",
    "def max_loudness_of_segment(seg, waveform, nmax=1): #determining max volume of a segment\n",
    "    try:\n",
    "        notebook.crop = seg\n",
    "        plot = waveform.crop(seg, return_data=False)\n",
    "        return pd.Series(plot.data.squeeze()).abs().sort_values().tail(nmax).mean()\n",
    "    except Exception as e:\n",
    "        print(e, waveform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NoHuB-Sf305"
   },
   "outputs": [],
   "source": [
    "def diarizator(fpath, speakers): #diarization function\n",
    "    try:\n",
    "        clusterer = KMeans(n_clusters=int(speakers)) \n",
    "        OWN_FILE = {'audio': fpath}\n",
    "        waveform = RawAudio()(OWN_FILE)\n",
    "        print(waveform)\n",
    "        diarization = pipeline(OWN_FILE)\n",
    "\n",
    "        segments = list(diarization.itersegments())\n",
    "\n",
    "        segments_loudnesses_df = pd.Series([max_loudness_of_segment(seg, waveform, 3) for seg in segments])\n",
    "        cluster_ids = clusterer.fit_predict(segments_loudnesses_df.to_numpy().reshape(-1, 1)).tolist() #clustering by volume\n",
    "\n",
    "        diarization_di = diarization.for_json()\n",
    "        for diarization_seg, cluster_id in tqdm(zip(diarization_di[\"content\"], cluster_ids)):\n",
    "            seg_length = diarization_seg[\"segment\"][\"end\"] - diarization_seg[\"segment\"][\"start\"]\n",
    "            label = cluster_id if seg_length > 0.4 else None\n",
    "            diarization_seg[\"label\"] = label\n",
    "        return diarization_di\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuA_isx39gMy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df['recognised'] = new_df.apply(lambda x: diarizator(x['file_path'], x['speakers']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very simple metrics here (for primary evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFEYKe3x_ur7"
   },
   "outputs": [],
   "source": [
    "def len_source(dictionary):\n",
    "    counter = 0\n",
    "    for i in dictionary:\n",
    "        counter+= len(dictionary[i])\n",
    "    return counter\n",
    "\n",
    "def len_regignised(lst):\n",
    "    try:\n",
    "        return len(lst)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "new_df['segs_source'] = new_df['source_data'].apply(len_source) #counting in source file\n",
    "new_df['segs_recognised'] = new_df['recognised'].apply(len_regignised) #counting in source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMgU0ajUA2w-"
   },
   "outputs": [],
   "source": [
    "new_df['distance'] = new_df['segs_source']-new_df['segs_recognised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4ep53lecllD"
   },
   "outputs": [],
   "source": [
    "def speakers_rec(dictionary):\n",
    "    lst = []\n",
    "    try:\n",
    "        for i in dictionary:\n",
    "            for j in i:\n",
    "                if j == 'label':\n",
    "                    lst.append(i[j])\n",
    "        return len(set(lst))\n",
    "    except:\n",
    "        return 0\n",
    "new_df['speakers_rec'] = new_df['recognised'].apply(speakers_rec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJUJwsfrhawE"
   },
   "outputs": [],
   "source": [
    "new_df['speaker_distance'] = new_df['speakers']-new_df['speakers_rec'] #distance between the number of speakers\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAtFIXegh7lx"
   },
   "outputs": [],
   "source": [
    "new_df.to_csv('diarization.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq8AkBjQie5l"
   },
   "source": [
    "# Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "54FewqkniiAO",
    "outputId": "a7674439-014c-4710-91c1-37149610fdb4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "x = [list(new_df['distance'].values),\n",
    "     list(new_df['speaker_distance'].values)]\n",
    "df = pd.DataFrame(x, index=['Distance in the number of segments', 'Distance in the number of speakers'])\n",
    "\n",
    "\n",
    "df.T.boxplot(vert=False, figsize=(20,10))\n",
    "plt.subplots_adjust(left=0.25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lk3JBaGyTzYU"
   ],
   "name": "diarization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
